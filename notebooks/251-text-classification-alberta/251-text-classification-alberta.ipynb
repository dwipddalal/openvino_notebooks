{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alberta Text Classification with OpenVINO\n",
    "\n",
    "This notebook shows a text classification with OpenVINO. We use [Alberta](https://huggingface.co/textattack/albert-base-v2-MRPC?text=I+like+you.+I+love+you) model is available from HuggingFace. OpenVINO is a powerful framework that can help optimize and accelerate machine learning models, making them more efficient and faster. By using OpenVINO, we can improve the performance of our text classification model and make it more accessible for deployment on various platforms.\n",
    "\n",
    "The Alberta model is a pre-trained language model that has been fine-tuned on the Microsoft Research Paraphrase Corpus (MRPC) dataset. It has been trained to classify whether the input sentence is paraphrased or not. We will be using this model to classify text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/albert-base-v2-MRPC\")\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/albert-base-v2-MRPC\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Alberta to OpenVINO IR\n",
    "\n",
    "![conversion_pipeline](https://user-images.githubusercontent.com/29454499/211261803-784d4791-15cb-4aea-8795-0969dfbb8291.png)\n",
    "\n",
    "For starting work with Alberta model using OpenVINO, model should be converted to OpenVINO Intermediate Represenation (IR) format. HuggingFace provided Alberta model is a PyTorch model, which is supported in OpenVINO via conversion to ONNX. We will use HuggingFace transformers library capabilities to export model to ONNX. `transformers.onnx.export` accepts preprocessing function for input samples (tokenizer in this case), instance of model, ONNX export configuration, ONNX opset version for export and output path. More information about transformers export to ONNX can be found in HuggingFace [documentation](https://huggingface.co/docs/transformers/serialization).\n",
    "\n",
    "While ONNX models are directly supported by OpenVINO runtime, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    "The `mo.convert_model` python function can be used for converting model with [OpenVINO Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Python_API.html). The function returns instance of OpenVINO Model class, which is ready to use in Python interface. However, it can also be serialized to OpenVINO IR format for future execution using `openvino.runtime.serialize`. In this case, `compress_to_fp16` parameter is enabled for compression model weights to `FP16` precision and also specified dynamic input shapes with possible shape range (from one token to maximum length defined in the processing function) for optimization of memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openvino.runtime import serialize\n",
    "from openvino.tools import mo\n",
    "from transformers.onnx import export, FeaturesManager\n",
    "\n",
    "# define path for saving onnx model\n",
    "onnx_path = Path(\"model/alberta.onnx\")\n",
    "onnx_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# define path for saving openvino model\n",
    "model_path = onnx_path.with_suffix(\".xml\")\n",
    "\n",
    "# get model onnx config function for output feature format casual-lm\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(pt_model, feature='sequence-classification')\n",
    "\n",
    "# fill onnx config based on pytorch model config\n",
    "onnx_config = model_onnx_config(pt_model.config)\n",
    "\n",
    "# convert model to onnx\n",
    "onnx_inputs, onnx_outputs = export(tokenizer, pt_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)\n",
    "\n",
    "# convert model to openvino\n",
    "ov_model = mo.convert_model(onnx_path, compress_to_fp16=True)\n",
    "\n",
    "# serialize openvino model\n",
    "serialize(ov_model, str(model_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "We can start by building an OpenVINO Core object. Then, read the network architecture and model weights from the .xml and .bin files, respectively. Finally, we compile the model for the desired device. Since we use the dynamic shapes feature, which is only available on CPU, we must use CPU for the device. Dynamic shapes support on GPU is coming soon.\n",
    "\n",
    "Since the text recognition model has a dynamic input shape, you cannot directly switch device to GPU for inference on integrated or discrete Intel GPUs. In order to run inference on iGPU or dGPU with this model, you will need to resize the inputs to this model to use a fixed size. Then, try running the inference on GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "# initialize openvino core\n",
    "core = Core()\n",
    "\n",
    "# read the model and corresponding weights from file\n",
    "model = core.read_model(model_path)\n",
    "\n",
    "# compile the model for CPU devices\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input keys are the names of the input nodes and output keys contain names of the output nodes of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing model\n",
    "\n",
    "NLP models often take a list of tokens as a standard input. A token is a single word mapped to an integer. To provide the proper input, we use a vocabulary file to handle the mapping. So first let us load the vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inference(sentence, inf_model):\n",
    "    \"\"\"\n",
    "    Run inference on the pre-trained ALBERT model for sentence classification.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence to classify.\n",
    "        \n",
    "    Returns:\n",
    "        predicted_label (int): Predicted label for the input sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode sentence with tokenizer\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    inputs = dict(inputs)\n",
    "    del inputs['token_type_ids']\n",
    "    \n",
    "    # run inference on model\n",
    "    output_key = inf_model.output(0)\n",
    "    outputs = inf_model(inputs)[output_key]\n",
    "    \n",
    "    # get predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits).item()\n",
    "    \n",
    "    # return predicted label\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify an example sentence\n",
    "sentence = \"This is an example sentence.\"\n",
    "predicted_label = inference(sentence, compiled_model)\n",
    "print(predicted_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Post-training Quantization API\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "We will use 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize YOLOv7.\n",
    "\n",
    "> **Note**: NNCF Post-training Quantization is available as a preview feature in OpenVINO 2022.3 release. Fully functional support will be provided in the next releases.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a Dataset for quantization.\n",
    "2. Run `nncf.quantize` for getting an optimized model.\n",
    "3. Serialize an OpenVINO IR model, using the `openvino.runtime.serialize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import nncf \n",
    "import numpy as np\n",
    "\n",
    "# prepare sample data\n",
    "sentence = \"This is an example sentence.\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# create a TensorDataset with the input IDs and attention mask\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "# create a DataLoader to load the sample data\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "quantization_dataset = nncf.Dataset(dataloader)\n",
    "\n",
    "quantized_model = nncf.quantize(pt_model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED)\n",
    "\n",
    "serialize(quantized_model, 'model/quantized_alberta.xml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nncf.quantize` function provides interface for model quantization. It requires instance of OpenVINO Model and quantization dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Quantized model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_compiled_model = core.compile_model(quantized_model)\n",
    "predicted_label = inference(sentence, int8_compiled_model)\n",
    "print(predicted_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "Finally, use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the `FP32` and `INT8` models.\n",
    "\n",
    "> **NOTE**: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP32 model (OpenVINO IR)\n",
    "!benchmark_app -m model/alberta.xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "!benchmark_app -m model/quantized_alberta.xml -d CPU -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
